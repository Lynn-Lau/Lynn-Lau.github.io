## 糗事百科爬虫
本文件夹创建的爬虫都是抓取的糗事百科网站，[糗事百科](http://www.qiushibaike.com/)。
### Qiu.py 功能介绍
此文件实现的糗事百科内容的抓取，内容简单，没有交互，所有的内容均是一次性抓取出来呈现在页面上，仅能抓取固定的某一页。
可以抓取的内容有：

* 每一个段子作者的用户名；
* 每一个段子的具体内容；
* 每一个段子获得赞数；

主要练习爬虫的内容有：

* 通过构造User-Agent将程序伪装成为浏览器；
* 利用try-except语句来捕获URLError；
* 爬虫模块中re库的正则表达式学习；

``` python
#-*- coding:utf-8 -*-
"""
Try to fetch Qiushibaike content

Author:lynn-lau
IDE:Pycharm 5.0
Language:Python 2.7
Date:2016-05-30
"""
import re
import urllib
import urllib2

# Using 'use_agent' and 'headers' to pretend it is a Browser
user_agent = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36"
headers = {'User-Agent':user_agent}

url ='http://www.qiushibaike.com/'

try:
    request = urllib2.Request(url,headers=headers)
    response = urllib2.urlopen(request)
    the_page = response.read()
    # print the_page
    # remind you that you have Successfully got them
    print "Congratulations! You have already fetched the whole content of the Web!!!"

    content = the_page.decode('utf-8')

    # Make a rule
    pattern = r'<div.*?author.*?>.*?<h2>(.*?)</h2>.*?<div.*?content">(.*?)</div>' \
              r'.*?<i.*?number">(.*?)</i>'
    program = re.compile(pattern,re.S)
    items = re.findall(program,content)
    for item in items:
        # item[0] is user name，item[1] is the content，item[2] is the votes
        print item[0],item[1],item[2]

    #Tell me why if there is some thing wrong with it
except urllib2.URLError,e:
    if hasattr(e,'code'):
        print e.code
    if hasattr(e,'reason'):
        print e.reason
else:
    pass


```

### QSBK.py 功能介绍

#### 本次更新
此脚本代码是对上一个脚本代码进行的优化，同样是对糗事百科内容的抓取。此脚本代码对功能进行了优化，如下：

* 添加了程序的交互功能；
* 且次脚本的代码是面向对象的编程方式，引出了类和方法，对代码进行了优化和封装，
* 实现了对多个页面的段子的抓取，每按一次回车键加载一个段子；

#### 不足之处

* 对于包含图片的段子只能加载配图的文字，图片未能加载；

附代码：

``` python
#-*- coding:utf-8 -*-
"""
Try to fetch the content of QSBK

Author:lynn-lau
Date:2016-05-31
IDE:Pycharm 5.1
Language:Python 2.7
"""
import re
import urllib
import urllib2
import thread
import time

# creat the QSBK class
class QSBK:
    # initiate the theory and define some variates
    def __init__(self):
        self.pageIndex = 1
        # define headers and pretend it is a browser
        self.user_agent = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 ' \
                          '(KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36'
        self.headers = {'User-Agent':self.user_agent}
        # define stories[] to storage the story
        self.stories = []
        # decide whether continue to store stories into the storage
        self.enable = False

    # define a theory which can fetch the whole code f the web
    def getPage(self,pageIndex):
        try:
            url = 'http://www.qiushibaike.com/hot/page/' + str(pageIndex)
            request = urllib2.Request(url,headers=self.headers)
            response = urllib2.urlopen(request)
            the_page = response.read()
            # decode the web code into utf-8
            pageCode = the_page.decode('utf-8')
            return pageCode

        # try to fetch what's wrong when we can connect to the server
        except urllib2.URLError,e:
            if hasattr(e,"reason"):
                print "There is something wrong when connected to QSBK",e.reason
                return None

    # Using regular to match the content we need and store them into the stories
    def getPageItems(self,pageIndex):
        pageCode = self.getPage(pageIndex)
        if not pageCode:
            print "Something wrong when loading the pageCode..."
            return None

        # that 's the regular used to match the content
        pattern = r'<div.*?author.*?>.*?<h2>(.*?)</h2>.*?<div.*?content">(.*?)</div>' \
                  r'.*?<i.*?number">(.*?)</i>'
        program = re.compile(pattern,re.S)
        # find all content which matched the regular
        items = re.findall(program,pageCode)
        pageStories = []
        for item in items:
            # item[0] is username, item[1] is the story,item[2] is the vote number
            pageStories.append([item[0],item[1],item[2]])
        return pageStories

    # when the first page comes to the end try to fetch the next page
    def loadPage(self):
        # when decided continue to store the stories into the storage
        if self.enable == True:
            # decide the time when continue to store the stories
            if len(self.stories) < 2:
                # if the number of stories less than 2, than we try to fetch the next page's
                # content, match the stories and store them into the storage
                pageStories = self.getPageItems(self.pageIndex)
                if pageStories:
                    self.stories.append(pageStories)
                    # page number adds
                    self.pageIndex += 1

    # try to load the story one by one by one
    def getOneStory(self,pageStories,page):
        for story in pageStories:
            # fetch the input from the keyboard
            input = raw_input()
            self.loadPage()
            if input == "Q":
                self.enable = False
                return
            #print "OK"
            print "Here comes the story\n",story[0],story[1],story[2]

    # 
    def start(self):
        print "We are loading the stories,press 'Enter' check the new story,press 'Q' quit..."
        self.enable = True
        self.loadPage()
        nowPage = 0
        while self.enable:
            if len(self.stories) > 0:
                pageStories = self.stories[0]
                nowPage += 1
                del self.stories[0]
                self.getOneStory(pageStories,nowPage)

spider = QSBK()
spider.start()
```